# 高性能融合网络TCP/IP通信协议的设计与实现 #

## 摘要 ##

## 引言 ##
计算机应用技术的不断发展，使得高性能计算应用范围不断扩大，已逐渐从科学计算领域，扩展到产品设计和大数据处理[pHPC平台TCP/IP通信优化关键技术研究]。随着云计算技术的兴起，高性能计算和云计算两个领域开始走向融合。一方面，高性能计算领域成熟的加速计算和网络拓扑给云计算带来了技术支撑。另一方面，云计算领域成熟的虚拟化技术和资源管理手段给高性能计算机的设计带来了新的可能。这种融合的趋势对高性能融合互连网络通信系统提出了新的需求和挑战。本文将设计并实现一套机制，为融合互连网络增加透明的高性能TCP/IP通信功能。

### 高性能计算和云计算的融合 ###

#### 高性能计算 ####
计算机淡诞生以来,运算速度一直是其发展的主要驱动力。计算机经历了大/巨型机、MPP、集群等发展阶段，性能平均每4年增长10倍。图[]展示了全球高性能计算机TOP500的性能变化趋势。十九世纪八十年代的巨型机，采用向量处理单元和SMP(Symmetric Multi-Processing, 对称多处理机系统)节点控制器，运行多线程的操作系统，以机柜的形式展现在人们面前。但它采用的向量化编程方式，编程难度大。上世纪九十年代的MPP（Massive Parallel Processing, 大规模并行处理）超级计算机，采用微处理器和NIC控制器，运行主从式操作系统，支持MPI(Message Passing Interface, 消息传递接口)编程，通常以机仓的形式呈现。2000年的刀锋式高性能计算机，采用集群的架构，使用集群管理软件管理，由multi-stage网互连。2010年的HPP体系结构，采用异构的计算单元和GAS(Global Address Space, 全局物理地址空间)。目前，我国的高性能计算机正在走向100PF(十亿亿次)，甚至艾级（百亿亿次）的量级。

![计算机量级图]()

高性能计算机的传统应用领域主要包括科学和工程计算，如大气模拟、数值分析等。主要运用于研究所、高校和计算中心等场所。

#### 云计算 ####
近年来，大数据的发展如火如荼。大数据的发展推动了云计算发展的进程，并引起了学术界和工业界的广泛关注。云计算中的虚拟化技术在数据中心的构建起到了重要的作用，通过系统虚拟化，单一计算机的多个虚拟机可以整合多台物理机器的功能。云计算的运用场景主要是公司企业，如百度云、阿里云，以及各地云计算中心，如北京云计算中心、南京云计算中心等。

虚拟化技术带来了管理上的方便，但其本身带来了性能损失，无法满足多种业务的需求。尤其是大规模机器学习模型的应用特征更加类似于高性能计算，需要高并发的计算和通信。云计算环境下海量数据的通信，也离不开高性能的通信系统。文献[用户级通信协议BCL-3对IP协议支持的研究]指出，集群系统只有10%用来作科学计算，其余80~90%都用于事务处理和信息服务。在大数据时代，云计算中心、数据中心需要借助高性能计算硬件平台，利用平台提供的高效网络、计算、容错和存储资源，加速信息处理。

#### 融合互连网络通信系统 ####
针对二者融合的趋势，中国科学院计算技术研究所设计实现了高性能融合互连网络通信系统，简称融合互连网络通信系统。

### 与TCP/IP融合 ###
云计算中心、数据中心的基于IP协议的应用程序和协议栈并不能直接运行在融合互连网络通信系统之上。由于应用角度的差异，两者编程思路和编程接口差异很大。目前的数据中心和云计算中心大量使用IP应用程序，采用Socket等技术，基于TCP/IP及其衍生的应用层协议栈实现。具有代表性的IP应用程序如：基于HTTP协议的浏览器和网络应用,基于FTP协议的文件分享站点,基于SMTP、POP3协议的邮件应用以及常用的telnet,DNS服务等。

让融合控制器在融合高性能计算和云计算的同时，进一步与以太网融合，就是搭建了前瞻技术和成熟技术之间的桥梁。从而使现有IP应用程序和协议栈不做任何更改便能利用高性能平台带来的带宽和性能优势。这就是本课题的意义。如何实现与以太网的融合，就是本文需要解决的问题。

### 相关术语 ###
本小节介绍文中讨论时的相关约定和缩写。

8个位的量在计算机学科被称为字节（byte），但网络相关的文献中被称为octet。我们沿用这个传统。

当涉及TCP/IP网络协议的分层时，使用基于OSI参考模型的通称Ln协议术语。如L2、L3和L4分别用来表示链路层、网络层和传输层。多数情况下，L2为以太网的同义词，L3指IPv4或IPv6，L4只UDP、TCP、ICMP。

![术语缩写表]()

### 主要工作及贡献 ###
本文以融合互连网络通信系统为基础，分析了TCP/IP在Linux中的实现原理，设计并实现了一套机制，使融合互连网络通信系统透明地支持TCP/IP协议及上层应用。具体包括：

1. 调研了融合互连网络通信系统的设计架构，分析了软件部分的BCL通信库实现原理，同时深入探讨Linux网络子系统的工作原理和TCP/IP协议栈的实现，找到BCL通信和TCP/IP通信的差异。

2. 设计并实现了虚拟以太网卡YAVIS，衔接上述差异。YAVIS作为Linux模块被插入到操作系统中，基于kBCL（内核BCL）通信库接口，完成了封装分发TCP/IP数据帧的功能，从操作系统驱动层为融合互连网络通信系统增加了TCP/IP支持。性能分析结果表明，YAVIS通信延迟相比于普通千兆网卡有一个数量级的提升（十微秒级），带宽较普通千兆网卡提升近2倍。

3. 使用轮询方式优化了YAVIS性能。采用了两种轮询方式：高精度定时器轮询和NAPI轮询。前者具有较高的灵活性，后者可以获得更高的通信性能。

4. 研究了Linux内核模块编程和调试方法，分析了日志调试、SystemTap调试工具和KGDB交互式调试工具在内核编程中的使用。

### 组织结构 ###
本文致力于为融合互连网络通信系统提供TCP/IP协议，设计并实现虚拟以太网卡YAVIS，并对其性能进行了评测和分析。本文共分六章。


## 相关研究 ##
本节将分为纵向和横向两部分，分别介绍本课题的研究基础和国内外相近的研究。研究基础将包括TCP/IP和高性能通信系统及其相关话题。剩余篇幅将介绍Infiniband标准里解决与以太网融合问题所采用的IPoIB和EoIB技术。

### TCP/IP通信技术简介 ###
TCP/IP作为最重要的网络协议，也是第一个被规范化定义的网络协议，是网际协议族的代名词。所以通常TCP/IP有两层内涵：狭义上指Transmission Control Protocol（传输控制协议）和Internet Protocol（网际协议）的组合，广义上指Internet Protocol suite（网际协议族）。网际协议族是包含传输控制协议和网际协议的更大的概念。在本文中，TCP/IP指代后者。以太网是TCP/IP的一种实现。

TCP/IP作为典型的分层协议，具有如图[]四层结构。

![TCP/IP层次结构](img/tcpip-arch.png)

应用层提供了不同主机间（也可以是同一个主机）不同应用程序之间传递用户数据的场所。这些应用程序利用下层如传输层提供的可信或不可信通信管道，与其它进程通信。参与通信的对象按照应用架构分成诸如C/S、P2P等网络结构。运作在这一层的协议主要包括SMTP、FTP、SSH、HTTP等，同时支撑着WWW和E-MAIL等涵盖互联网几乎所有领域的服务。应用层寻址方式通过端口方式进行。

传输层进行主机到主机的通信。这里的主机可以是同一台机器，也可以是不同的机器。如果是不同的机器，它们可以在一个本地网络中，也可以在路由器连接的不同网络中。传输层为应用的通信需求提供了通道。传输层常用的两种协议分别是UDP和TCP。前者提供不可靠的数据报服务，后者提供流控、连接建立和可靠数据通信。

网际层负责在各个网络边界处完成数据报的交换。它采用单一的网络接口来屏蔽低层次的真实拓扑细节。TCP/IP协议族的寻址和路由都是在这一层完成的。IP是网际层最重要的协议。它使用IP地址完成寻址。网络层通过地址解析协议(ARP)和反向地址解析协议(RARP)完成IP地址与下层MAC地址的转换过程，是网际层和链路层的桥梁。

链路层应用与本地网络内部的底层通信。这一层次包含描述本地网络拓扑结构的协议和传输数据到邻居节点的接口。链路层可以在网卡驱动程序、VPN或者专用芯片中实现。这一层次的寻址方式可以是MAC地址。

### TCP/IP协议在Linux内核中的实现 ###

## 虚拟以太网卡YAVIS的设计 ##
YAVIS是基于cHPP控制器的虚拟以太网卡，用来实现对TCP/IP的透明支持。

### YAVIS的上下文 ###

YAVIS采用BCL用户级通信库完成处理单元间的通信。底层硬件为cHPP控制器。YAVIS构成的通信系统如图所示：

![加入YAVIS的高性能通信系统](img/chpp-bcl-yavis.png)

cHPP控制器使用HT总线协议连接pHPC平台的各个处理单元，具有RDMA并行数据交换芯片，并统一pHPP超节点内所有处理单元的内存及IO设备的物理地址空间。

cHPP驱动是对cHPP控制器的软件抽象。提供内存管理等基础设施。在cHPP驱动的基础上，pHPC封装了一层BCL通信库，来支持通信。BCL通信库分为用户空间和内核空间两个版本，本文中如不加强调，BCL特指内核空间BCL，即kBCL。

## 内核开发调试方法研究  ##
本章讨论内核和内核模块的调试方法。本章是全文最精彩的部分，因为掌握了恰当的调试方法，才使得YAVIS的开发成为可能。GNU/Linux内核是一个复杂的系统，对于大多数内核开发人员，掌握一些调试的方法是完成开发的的关键步骤。

本章介绍在YAVIS开发过程中使用到的调试技术。着重介绍通过VMware虚拟机软件和'KGDB'调试器，对目标机的内核和动态加载的模块进行串口调试的过程。使用的主机本身即为运行着Linux操作系统的物理机器，这种情况资料匮乏，大多数文档描述两台物理地址或两台虚拟机联机调试的方法。通过试错实验，总结经验作为本章主要内容。

### 日志调试 ###
在谈论内核调试器之前，我们先了解一下内核本身提供的一种日志调试方法（函数）:printk。缜密设计出的printk可以在内核代码的任何地方（包括原子上下文）使用，向一个环形缓冲区中输入调试信息,并支持与应用层标准库中printf函数一致的格式化字符串。日志可以由dmesg命令查看。printk共有8个日志级别(0~7,数字越大，级别越低)，方便筛选开发者关注的调试信息。

printk的好处在于哪里都可以调用，简单不需要对内核做任何配置修改和环境搭建。但是，它的弊端并没有被它带来的方便所掩盖。首先，printk本身一种日志调试，而在软件工程中，日志调试通常是软件开发后期用来分析性能瓶颈的。对于一个正在开发中的内核项目，它的调试效率极低。因为在每次排查一个内核崩溃错误时，通常要在代码中插入多个printk，运用二分法多次尝试才有可能定位到错误位置。正因如此，随处插入printk影响了编码风格，为后期清理增加了工作。其次，日志缓冲区有限且易失，一旦内核发生严重错误当机，是难以查看导致内核当机时状态的。

printk作为调试手段之一，是一种静态分析程序运行流程和中间状态的方法，而我们需要一种强大的交互式调试器。交互的含义我认为，是能使我们能像使用GDB那样单步执行我们的内核程序，让我们深入分析内核每一步执行时的状态。这样的调试器有多种，本章剩余部分将会详细描述其中一种——KGDB的配置和使用。

### KGDB ###
KGDB在2.6.26-RC5内核版本是被加入内核树主分支。在这之前作为补丁出现。[http://elinux.org/Kgdb]。也就是说KGDB是新版本内核现成的功能，然而这并不意味着开发者什么都不用做就使用KGDB。内核级的调试器，采用双机调试的方式。作为本章范围内的约定，运行打开KGDB调试器的内核叫做目标内核，机器叫做目标机，也叫TARGET。运行GDB通过串口连接并调试目标机的机器我们约定为主机HOST。

#### 实验环境 ####
(这里应该形成一张表)
主机: 物理机器，Ubuntu14.04x86_64
目标机: 虚拟机(in VMware 6.0)，CentOSx86_64，KernelVer=2.6.32

注意主机和目标机处理机架构要一致（这里都是x86_64），否则HOST的gdb无法识别TARGET的内核二进制文件的格式，强行调试的话没准得整个交叉编译工具链里的gdb。

#### 构建目标机 ####

编译内核之前，通过配置将下列开关打开，包括魔法键开关、调试信息开关和KGDB开关：

    ...
    CONFIG_MAGIC_SYSRQ=y
    ...
    CONFIG_DEBUG_INFO=y
    ...
    CONFIG_KGDB=y
    CONFIG_KGDB_SERIAL_CONSOLE=y
    ...

再将R(ead)O(nly)DATA开关关闭，因为这个选项的作用是保护内核数据结构不被第三方访问或修改，而观察和修改数据正是我们调试内核时要做的 —— 例如我们需要打断电暂停内核的执行来观察变量。

    # CONFIG_DEBUG_RODATA is not set

编译安装所用命令：

    make -j4 && make -j4 modules && sudo make modules_install \
                        && sudo make install && make -B vmlinux
                    
这串指令将编译内核和模块，然后安装内核和模块。最后一个vmlinux需要留心，主机的gdb就是从这里获得内核符号的。/boot/有同名二进制镜像，不过那是是精简压缩过，包含信息过少，不能被调试过程使用。

#### 连接主机和目标机 ####

我使用了VMware提供的虚拟串口和主机通信，完成“远程“调试。首先需要确保虚拟机处于关闭状态，再在硬件配置里为虚拟机添加Serial Port。

![](img/add-serial-port.png)

因为调试需要交互，使用第三种｀Use Socket(named pipe)｀连接方式。文件路径和名称可以自定义。核心技术是｀from server to application｀要选对。操作完成后，启动虚拟机系统，/tmp/中出现了dbg_pipe文件。

下面我们测试联通性。

    # 在主机上执行 #
    socat /tmp/dbg_pipe TCP4-LISTEN:9001　#将文件映射到一个端口
    telnet 127.0.0.1 9001

    # 在目标机上执行 #
    sudo chmod 222 /dev/ttyS1
    echo 'hello' > /dev/ttyS1
    
这时在HOST的telnet回话中会显示hello。这里的ttyS*到底是几每个机器不一样。在VMware创建虚拟机的默认过程，串口打印机占用了ttyS0。所以在默认情况下，对于新装的虚拟机，第一个手动添加串口设备文件对应ttyS1。

反过来，从HOST向TARGET传送数据：

    # 在目标机上执行 #
    sudo cat /dev/ttyS1

此时在主机上的telnet会话中敲入一些字符，这些字符将在TARGET的终端里显示。

#### 调试内核 ####
注意：执行上述操作前确认socat /tmp/dbg_pipe TCP4-LISTEN:9001在执行。

首先，修改内核参数/boot/grub/目录中的grub.cfg或是menu.lst（需要root权限）。在kernel或linux指令后追加参数：kgdboc=ttyS1,115200。有时候我们需要调试内核启动过程，需要内核等着调试器接管后再启动，这种情况下可以加kgdbwait参数。这样系统启动时，如果没有调试器接入并发送c(ontinue)命令，就停在那儿等。

进入系统后，我们需要断下内核。

    # 在目标机上执行 #
    sudo chmod 222 /proc/sysrq-trigger
    echo g > /proc/sysrq-trigger
    
这样，内核就会暂停运行，等待调试器接管、给出指令。

    # 在主机上执行 #
    gdb ./vmlinux
    ...
    (gdb) set serial baud 115200
    (gdb) target remote localhost:9001
    
vmlinux就是上面编译安装部分讲到的生成的vmlinux，我们需要把它从TARGET中拷贝到HOST上。

#### 模块调试 ####
我们知道Linux是宏内核，内核和所有模块都运行在同一地址空间，这为我们调试内核提供了便利。但是，我们还是不能直接用上面的方法调试内核模块。如果模块中提供了一个foo函数，调试者直接break foo是没法在这个函数的入口处打断点的，因为系统找不到符号。GDB是从vmlinux文件中获得符号，而模块的符号并不包含在vmlinux中（vmlinux建立时模块可能都不存在）。模块的符号都存在ko文件里，我们需要导入这个文件。

    # 在GDB中执行 #
    (gdb) add-symbol-file <your-module.ko> <address>

其中<your-module.ko>就是编译后模块文件的名称，`<address>`是模块代码段的加载地址。GDB就是通过这个偏移量算出每个符号在运行内核中的地址。模块加载地址这样查看：

    sudo cat /sys/module/<your-module>/sections/.text


#### 降低优化程度 ####
至此，调试环境基本搭建完成。但是由于内核编译时进行了，破坏了二进制代码和源代码对应关系。下面提供了一种降低优化程度的办法。

    # Makefile里头加这个 #
    ifeq ($(DEBUG),y)
        DEBFLAGS = -O -g -DSBULL_DEBUG
    else
        DEBFLAGS = -O2
    endif
    EXTRA_CFLAGS += $(DEBFLAGS)
    
应当我们最多可以将优化降低至O1级。完全关闭优化，会导致内核变异出错。因为内核中用到了一些优化带来的特性，如展开内联函数。

### SystemTap ###
SystemTap是一个Linux调试和性能分析工具[SystemTap: Instrumenting the Linux Kernel for
Analyzing Performance and Functional Problems]，可用于应用层和内核层的分析，但主要侧重内核层。SystemTab可以在不修改内核代码、不重复编译内核、不重启机器的情况下，收集运行内核的信息并使信息可视化。调试人员可以利用它绘制函数调用关系图，打印寄存器信息和调用栈，输出内核中指定变量（可以是局部变量）。它如同一个更加方便prink，方便调试人员观察内核行为，诊断错误异常，分析系统性能。在YAVIS开发过程中，我们使用SystemTap分析包发送和接收情况，并分析通信性能瓶颈。

#### SystemTap工作流程 ####
SystemTap使用了Kprobe技术探测内核信息，辅以Relayfs向用户传递消息。

SystemTap首先将SystemTap脚本文件翻译成C源文件。这个C源文件实际上是一个内核模块，实现了脚本文件中描述的功能。接着SystemTap编译源文件获得二进制模块文件，并动态加载模块。模块被载入运行内核后，会报告脚本文件中指定的一些事件。事件会触发脚本文件中编写的处理函数，执行相关操作。一般操作内容是：收集所需信息，并通过标准输出打印给用户。SystemTap会话结束于用户发出中断信号，即`Ctrl + C`，内核模块将随之被安全卸载。

![](img/systemtap.png)

SystemTap提供了一些内置函数，帮助我们快速开发测试脚本。常用的内置函数如：

- print(str) - 打印str的值
- printf(fmt) - 如同C语言的printf函数
- probefunc() - 返回当前探测函数的函数名
- execname() - 返回当前进程的名字
- pid() - 返回当前进程ID
- uid() - 返回当前进程用户ID
- cpu() - 返回运行当前进程的CPU号

另外一些内置功能以Tapset的形式出现。Tapset相当于SystemTap的库。它提供的功能不仅仅是函数，还包括一些预定义的探测点,如：

- timer.ms(N) - 每N毫秒探测一次（用于性能测试）
- begin - 探测模块加载时执行一次

当然，用户也可以开发自己的Tapset。

#### SystemTap脚本语言 ####
SystemTap语言是一种与C语言和awk语言类似的脚本语言。限于篇幅，这里并不系统地介绍SystemTap语言，而是结合毕业设计的调试场景，使用例子说明SystemTap的语法特征和编程结构。

    #!/usr/bin/stap
    probe module("yavis").function("*").call {
            printf("%s -> %s\n", thread_indent(1), probefunc())
    }
    probe module("yavis").function("*").return {
            printf("%s -> %s\n", thread_indent(-1), probefunc());
    }

上述代码的功能是输出YAVIS的代码调用关系图。第一行描述脚本采用的解释器是stap程序。第二行表示在yavis模块中所有函数中插入探针，并在这些函数调用时触发第三行的代码。第三行代码向标准输出打印一串信息，信息包括当前函数的名字，由内置的probefunc收集。第五至第七行代码与上面的类似，只是在函数返回时触发。这样，所有YAVIS模块的函数在调用时输出函数名，返回时再次输出函数名，同时由内置的thread_indent函数负责自动的缩进，最终绘制了整个YAVIS模块的函数调用关系图。

    #!/usr/bin/stap
    probe module("yavis").function("yavis_poll") {
    	   if ($revt->type == 0) {
    		  printf("-- package received --\n")
    		  printf("revt.msg_len = %d\n", $revt->msg_len)
    		  for (i = 0; i < $revt->msg_len; i++) {
    			 printf("%x", $revt->rbuff[i])
    		  }
    		  printf("\n")
        } else if ($sevt->type == 0) {
    		  printf("-- package sent --\n");
    	   }
    }
    
    probe module("yavis").function("yavis_tx") {
    	   printf("-- sending package --\n");
    	   printf("skb->len = %d\n", $skb->len)
    	   for (j = 0; j < $skb->len; j++) {
    	       printf("%x", $skb->data[j])
    	   }
    	   printf("\n")
    }

*上面这段脚本可以让我们在发送和接收过程中查看数据。注意结构体类型的数据，无论指针与否，一律使用`->`方式访问成员变量。*

(这里在加一个性能测量的例子)

## 性能测试及分析 ##

## 总结和展望 ##


---
现在，人们不再只追求性能，更追求效能和功能。高性能计算机的发展方向逐渐分化成面向E级的高性能计算技术和普及型计算技术。中国科学院计算技术研究所设计出了普及型个人高性能计算机pHPC(personal High Performance Computer)，基于超并行体系结构(Hyper Parallel Processing, HPP)和Xingzhe5单一系统镜像(Single System Image, SSI)系统，同时具有高性能计算、效用计算技术的特点。它具有高性能计算机的高性能特点，比普通工作站或PC以及网络计算机在浮点运算速度、能够处理的数据集大小、IO性能、数据交换性能和同步性能上，要好一个数量级以上。pHPC的设计的另一目标是具有个人计算机的低性能、低噪音、低成本、易用、易编程、易管理、高可靠和高效能，适合办公室使用环境、管理和编程比较简单。[PHPC一种普及型高性能计算机]。

[相关研究]高性能融合互连网络通信系统